Sequence-to-sequence, or Seq2Seq, problems have become a mainstay of modern deep learning. These are problems where the input is a non-empty variable-length sequence of tokens or data points, and our output is similarly a non-empty variable-length sequence of tokens. Many interesting and relevant problems can be phrased as sequence-to-sequence problems. For example, the canonical example of a such a problem is machine translation. Here, we are tasked with designing a system that is able to translate phrases or sentences in one language to another automatically. In this case, the input sequence is a sequence of words in our source language, while the output sequence is a sequence of words in our target language.

More generally, we can imagine different scenarios where we want to translate from one representation of data to another. For instance, part-of-speech tagging can be solved as a sequence-to-sequence problem, where the input sequence is a sequence of tokens, and the output sequence is the label for each token, such as a noun, verb, preposition, or something else. Other tasks can be considered as special cases of sequence-to-sequence tasks, such as sentiment analysis, where our output sequence is simply a single token denoting some metric of sentiment.

However, sequence-to-sequence tasks are not restricted to language related tasks. Arithmetic operations can be considered sequence-to-sequence problems, where the input sequence can be an array of numbers and the output is their sum, for instance. Some simple algorithmic examples include inputting an array of integers, and outputting the sorted array. Even more complex problems, such as the traveling salesman problem, can be thought of sequence-to-sequence problems. In this case, the input is a list of cities and their coordinates on a map, and the output is the ordering of the cities that minimizes the total distance traveled if we were to visit each city in the specified order. Finally, we need not have explicitly sequential data in order for us to use the techniques that are used to solve sequence-to-sequence problems. As long as we process our data or input sequentially, it possible to use these techniques to tackle these problems. For example, we can process images by sequentially considering pixels or adjacent regions of the image, and then label smaller parts of the image.

There are many more problems that can be cast, albeit sometimes inefficiently, as sequence-to-sequence problems. We encourage the reader to think of other problems that can be treated as sequence-to-sequence problems and to try using the techniques discussed in these following sections.

With these problems in mind, it is now important to consider classes of models that would be useful in tackling these problems. Recurrent neural networks, such as LSTMs, are well equipped for handling sequential data, such as text or time-series data. They are amenable to learning from variable-length sequences in the input, output, or both, making them suitable to apply to any of the problems described above. Moreover, as we've seen, LSTMs are designed to handle longer-term dependencies between the inputs, and this is especially important for some of the problems described above. With these preliminaries in place, we can introduce specific architectures used by researchers to solve these problems.

The most common approach in solving sequence-to-sequence network is to design an encoder-decoder network. With this, we use the first part of the network, or the encoder network, to compress the input sequence as a fixed-size embedding vector. We use the second part of the network, or the decoder network, to decode this embedding vector into the output sequence. A high-level diagram of this process can be seen in Figure X.
