{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch10_Generative.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbDJeqx0scdo3HPnNtP4eP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YSLSNJBYZyYh"},"source":["#Implementing a VAE"]},{"cell_type":"code","metadata":{"id":"UjQX09lMWloJ","executionInfo":{"status":"ok","timestamp":1637597081348,"user_tz":300,"elapsed":1499,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["import torch\n","from torch.distributions.multivariate_normal \\\n","  import MultivariateNormal\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","import torch.optim as optim"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AcuN707I6Qh3","executionInfo":{"status":"ok","timestamp":1637597081349,"user_tz":300,"elapsed":12,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"7d69de72-6f51-452d-a330-6d3c5a726a50"},"source":["# For reproducability\n","import random\n","random.seed(0)\n","\n","torch.manual_seed(0)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6ab21cba50>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"nAuimXkkW25x","executionInfo":{"status":"ok","timestamp":1637597081349,"user_tz":300,"elapsed":7,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["# Encoder layers (Gaussian MLP)\n","D_in, H, D_out = 784, 200, 20\n","input_layer = nn.Linear(D_in, H)\n","hidden_layer_mean = nn.Linear(H, D_out)\n","hidden_layer_var = nn.Linear(H, D_out)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBaMa1lFZ8bv","executionInfo":{"status":"ok","timestamp":1637597081349,"user_tz":300,"elapsed":6,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["# Decoder layers (Bernoulli MLP for MNIST data)\n","recon_layer = nn.Linear(D_out, H)\n","recon_output = nn.Linear(H, D_in)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ub2EFOT2aEzO","executionInfo":{"status":"ok","timestamp":1637597081349,"user_tz":300,"elapsed":6,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["class VAE(nn.Module):\n","  def __init__(self, D_in, H, D_out):\n","    super(VAE, self).__init__()\n","    self.D_in, self.H, self.D_out = D_in, H, D_out\n","\n","    # Encoder layers (Gaussian MLP)\n","    self.input_layer = nn.Linear(D_in, H)\n","    self.hidden_layer_mean = nn.Linear(H, D_out)\n","    self.hidden_layer_var = nn.Linear(H, D_out)\n","    \n","    # Decoder layers (Bernoulli MLP for MNIST data)\n","    self.recon_layer = nn.Linear(D_out, H)\n","    self.recon_output = nn.Linear(H, D_in)\n","    self.tanh = nn.Tanh()\n","    self.sigmoid = nn.Sigmoid()\n","    \n","  def encode(self, inp):\n","    h_vec = self.input_layer(inp)\n","    h_vec = self.sigmoid(h_vec)\n","    means = self.hidden_layer_mean(h_vec)\n","    log_vars = self.hidden_layer_var(h_vec)\n","    return means, log_vars\n","\n","  def decode(self, means, log_vars):\n","    # Reparametrization trick\n","    std_devs = torch.pow(2,log_vars)**0.5\n","    aux = MultivariateNormal(torch.zeros(self.D_out), \\\n","    torch.eye(self.D_out)).sample()\n","    sample = means + aux * std_devs\n","    \n","    # Reconstruction\n","    h_vec = self.recon_layer(sample)\n","    h_vec = self.tanh(h_vec)\n","    output = self.sigmoid(self.recon_output(h_vec))\n","    return output\n","\n","  def forward(self, inp):\n","    means, log_vars = self.encode(inp)\n","    output = self.decode(means, log_vars)\n","    return output, means, log_vars\n","\n","  def reconstruct(self, sample):\n","    h_vec = self.recon_layer(sample)\n","    h_vec = self.tanh(h_vec)\n","    output = self.sigmoid(self.recon_output(h_vec))\n","    return output"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Flp9U3ZFaKRX","executionInfo":{"status":"ok","timestamp":1637597081350,"user_tz":300,"elapsed":7,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["def compute_loss(inp, recon_inp, means, log_vars):\n","  # Calculate reverse KL divergence \n","  # (formula provided in Kingma and Welling)\n","  kl_loss = -0.5 * torch.sum(1 + log_vars \n","                            - means ** 2 - torch.pow(2,log_vars))\n","\n","  # Calculate BCE loss\n","  loss = nn.BCELoss(reduction=\"sum\")\n","  recon_loss = loss(recon_inp, inp)\n","  return kl_loss + recon_loss"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlIKK40zaN-m","executionInfo":{"status":"ok","timestamp":1637597081350,"user_tz":300,"elapsed":7,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["D_in, H, D_out = 784, 500, 20\n","vae = VAE(D_in, H, D_out)\n","vae.to(\"cpu\")\n","\n","def train():\n","  vae.train()\n","  optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n","\n","  train_loader = torch.utils.data.DataLoader(\n","      datasets.MNIST('../data', \n","                     train=True, \n","                     download=True, \n","                     transform=transforms.ToTensor()),\n","                     batch_size=100, \n","                     shuffle=True)\n","  \n","  epochs = 10\n","  for epoch in range(epochs):\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","      optimizer.zero_grad()\n","      data = data.view((100,784))\n","      output, means, log_vars = vae(data)\n","      loss = compute_loss(data, output, means, log_vars)\n","      loss.backward()\n","      optimizer.step()\n","      if (batch_idx * len(data)) % 10000 == 0:\n","        print(\n","            'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}' \\\n","            .format(\n","            epoch, batch_idx * len(data), len(train_loader.dataset),\n","            100. * batch_idx / len(train_loader), loss.item()))\n","  torch.save(vae.state_dict(), \"vae.%d\" % epoch)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sF1dK1BaSpg","executionInfo":{"status":"ok","timestamp":1637597081350,"user_tz":300,"elapsed":6,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["def test():\n","  dist = MultivariateNormal(torch.zeros(D_out), torch.eye(D_out))\n","  vae = VAE(D_in, H, D_out)\n","  vae.load_state_dict(torch.load(\"vae.%d\" % 9))\n","  vae.eval()\n","  outputs = []\n","  \n","  for i in range(100):\n","    sample = dist.sample()\n","    outputs.append(vae.reconstruct(sample).view((1,1,28,28)))\n","  outputs = torch.stack(outputs).view(100,1,28,28)\n","  save_image(outputs, \"prior_reconstruct_100.png\", nrow=10)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9j5PY5saVpY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637597244080,"user_tz":300,"elapsed":162735,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"e71d97f6-1783-4988-aca4-5d4e977ea535"},"source":["train()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [0/60000 (0%)]\tLoss: 55952.656250\n","Train Epoch: 0 [10000/60000 (17%)]\tLoss: 22144.039062\n","Train Epoch: 0 [20000/60000 (33%)]\tLoss: 20561.402344\n","Train Epoch: 0 [30000/60000 (50%)]\tLoss: 19222.226562\n","Train Epoch: 0 [40000/60000 (67%)]\tLoss: 19297.957031\n","Train Epoch: 0 [50000/60000 (83%)]\tLoss: 17642.384766\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 16681.759766\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 16511.546875\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 15210.477539\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 16148.983398\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 16288.840820\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 16266.238281\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 15242.084961\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 15064.274414\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 18326.287109\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 14104.389648\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 14261.106445\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 14967.324219\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 13167.408203\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 14198.993164\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 13529.751953\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 14154.439453\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 13469.017578\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 13866.195312\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 14206.777344\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 14302.945312\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 13048.618164\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 15418.697266\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 13730.386719\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 13107.153320\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 14269.023438\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 13693.005859\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 12458.881836\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 12944.063477\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 13673.570312\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 12701.249023\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 12595.849609\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 13707.856445\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 12847.349609\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 12781.701172\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 14270.169922\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 12348.052734\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 12927.649414\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 11896.478516\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 12941.965820\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 12311.972656\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 12250.702148\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 12588.513672\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 12503.103516\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 12679.904297\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 12621.823242\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 13550.755859\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 12948.678711\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 12120.589844\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 12816.015625\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 12815.571289\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 13297.917969\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 12054.651367\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 11642.633789\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 12168.153320\n"]}]},{"cell_type":"code","metadata":{"id":"65_wEyrL4L2g","executionInfo":{"status":"ok","timestamp":1637597244081,"user_tz":300,"elapsed":18,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":["test()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYtgl3O95QZx","executionInfo":{"status":"ok","timestamp":1637597244081,"user_tz":300,"elapsed":16,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"source":[""],"execution_count":10,"outputs":[]}]}