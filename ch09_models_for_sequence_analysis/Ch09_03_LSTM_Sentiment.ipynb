{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch09_03_LSTM_Sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "rXkUdXjimRyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- IMDB Movie Reviews, Binary Pos/Neg Sentiment\n",
        "- Prune vocabulary to 30,000 most common words\n",
        "- Pad each input sequence up to 500 words\n",
        "- Inputs should be 500 dim vectors where each element correspnds to the vocab index of the corresponding word"
      ],
      "metadata": {
        "id": "ToALc452WOJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "metadata": {
        "id": "XtJEcMDPuhTt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_iter = IMDB(split=('train'))"
      ],
      "metadata": {
        "id": "7-gu9IfAx1kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4337820f-c029-408e-fa32-428e3ca20dd1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:05<00:00, 14.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer and build vocabulary\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# build vocab from iterator and add a list of any special tokens\n",
        "text_vocab = build_vocab_from_iterator(yield_tokens(train_iter), \n",
        "                                       specials=['<unk>', '<pad>'])\n",
        "text_vocab.set_default_index(text_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "XU4RVcFJ3TSw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_vocab(tokenizer(\"Hello is it me you're looking for?\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCSX9_hFC4W_",
        "outputId": "74fbfd75-7991-443c-cf1d-29ec3b69a00d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4645, 10, 11, 78, 26, 9, 183, 296, 19, 55]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define pipelines\n",
        "def text_pipeline(x, max_size=512):\n",
        "   text = tokenizer(x)\n",
        "   \n",
        "   # reduce vocab size\n",
        "   pruned_text = []\n",
        "   for token in text:\n",
        "     if text_vocab.get_stoi()[token] >= 30000:\n",
        "       token = '<unk>'\n",
        "     pruned_text.append(token)\n",
        "   \n",
        "   # pad sequence or truncate\n",
        "   if len(pruned_text) <= max_size:\n",
        "     pruned_text += ['<pad>'] * (max_size - len(pruned_text))\n",
        "   else:\n",
        "     pruned_text = pruned_text[0:max_size]\n",
        "   return text_vocab(pruned_text)\n",
        "\n",
        "label_pipeline = lambda x: (0 if (x == 'neg') else 1)"
      ],
      "metadata": {
        "id": "_VWkiK8T0orQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test pipelines\n",
        "print(text_vocab.get_itos()[29999])\n",
        "print(text_vocab.get_itos()[30000])\n",
        "print(text_pipeline('hello, I saw the wanderings waned'))\n",
        "print(len(text_pipeline('hello, I saw the wanderings waned')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Clf31dYj8Uz5",
        "outputId": "3df76789-74bb-4e9e-83b8-f7f6061d9cb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wanderings\n",
            "waned\n",
            "[4645, 4, 13, 220, 2, 29999, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the data preparation, we need to serve minibatches of a desired size from the underlying dataset. We can use the built-in DataLoader class from PyTorch to sample teh dataset in batches.  Before we do so, we need to define a function, collate_batch, that will tell the DataLoader how to preprocess each batch."
      ],
      "metadata": {
        "id": "6A-WoVQeONNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define preprocessing\n",
        "def collate_batch(batch):\n",
        "  label_list, text_list = [], [] \n",
        "  for label, review in batch:\n",
        "    label_list.append(label_pipeline(label))\n",
        "    text_list.append(text_pipeline(review))\n",
        "  return (torch.tensor(label_list, dtype=torch.long),\n",
        "          torch.tensor(text_list, dtype=torch.int32))\n"
      ],
      "metadata": {
        "id": "cOSlnzk34DL8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collate_batch function simply runs the labels and review strings through each respective pipeline and returns the batch as a tuple of tensors (labels_batch, reviews_batch). Once the collate_fn is defined, we simply load the dataset and configure the dataloaders:"
      ],
      "metadata": {
        "id": "qt2kmzmVPGXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets and create dataloaders for batching\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_iter, val_iter = IMDB(split=('train','test'))\n",
        "trainloader = DataLoader(train_iter, \n",
        "                         batch_size = 4, \n",
        "                         shuffle=False,\n",
        "                         collate_fn=collate_batch)\n",
        "valloader = DataLoader(val_iter, \n",
        "                       batch_size = 4, \n",
        "                       shuffle=False,\n",
        "                       collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "jPpeIZkj2ZRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test pipelines and collate_batch()\n",
        "for labels, reviews in trainloader:\n",
        "  print(labels.shape)\n",
        "  print(reviews.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H86aQQb52gAo",
        "outputId": "c125a141-73e0-408f-c7d4-7df0e5ca70b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n",
            "torch.Size([4, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build TextClassification model"
      ],
      "metadata": {
        "id": "0TADZfe3AuXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is ready to go, we'll begin to construct the sentiment analysis model, step by step. First, we'll want to map each word in the input review to a word vector. To do this, we'll utilize an  embedding layer, which, as you may recall from the last chapter, is a simple lookup table that stores an embedding vector that corresponds to each word. Unlike in previous examples, where we treated the learning of the word embeddings as a separate problem (i.e., by building a Skip-Gram model), we'll learn the word embeddings jointly with the sentiment analysis problem by treating the embedding matrix as a matrix of parameters in the full problem. We accomplish this by using the PyTorch primitives for managing embeddings (remember that input represents one full minibatch at a time, not just one movie review vector):"
      ],
      "metadata": {
        "id": "qTIOsxspPiMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "X9TYzEQZBBTV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = nn.Embedding(\n",
        "                      num_embeddings=30000,\n",
        "                      embedding_dim=512,\n",
        "                      padding_idx=text_vocab.get_stoi()['<pad>'])"
      ],
      "metadata": {
        "id": "cFeRlnasBI7V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test embedding\n",
        "emb = embedding(torch.randint(high=29999,size=(4,500)))\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pGMoB_4EyRk",
        "outputId": "5c9ec69b-c662-40be-8783-d2068face76d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 500, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextClassifier,self).__init__()\n",
        "    self.layer_1 = nn.Embedding(\n",
        "                      num_embeddings=30000,\n",
        "                      embedding_dim=512,\n",
        "                      padding_idx=1)                      \n",
        "    self.layer_2 = nn.LSTMCell(input_size=512, hidden_size=512)\n",
        "    self.layer_3 = nn.Dropout(p=0.5)\n",
        "    self.layer_4 = nn.Sequential(\n",
        "                      nn.Linear(512, 2),\n",
        "                      nn.Sigmoid(),\n",
        "                      nn.BatchNorm1d(2))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.layer_1(x)\n",
        "    x = x.permute(1,0,2)\n",
        "    h = torch.rand(x.shape[1], 512)\n",
        "    c = torch.rand(x.shape[1], 512)\n",
        "    for t in range(x.shape[0]):\n",
        "      h, c = self.layer_2(x[t], (h,c))\n",
        "      h = self.layer_3(h)\n",
        "    return self.layer_4(h)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jv7SjkPz7TlY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop\n",
        "\n",
        "*   The training loop takes approximately 2 minutes per batch.  Therefore, we set N_EPOCHS = 1 and break after the first batch so you can execute the file.\n",
        "*   To do a complete training, set N_EPOCHS = 40 and remove the break statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZkangJ-D_2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim "
      ],
      "metadata": {
        "id": "s1n9XtjL69j5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 1\n",
        "model = TextClassifier()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  running_loss = 0\n",
        "  for labels, inputs in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    # This code takes about 2 min per batch\n",
        "    # We'll stop after first batch\n",
        "    break\n",
        "  print(f'Epoch: {epoch} Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyrhBxngD78A",
        "outputId": "40ae779b-713f-4d23-e78f-f2bc58a0a534"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 1.0499241352081299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nRg09KYCQdP_"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}