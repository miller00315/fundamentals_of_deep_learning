{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch09_04_Dissecting_NTN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation\n",
        "First, you'll need to download the Spacy NLP library. Run the next cell to install Spacy, then **RESTART the COLAB RUNTIME by clicking on the menu Runtime - Restart runtime.**"
      ],
      "metadata": {
        "id": "6lPhLWeAwK0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Spacy tokenizers\n",
        "#!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "# Need to restart Colab Runtime"
      ],
      "metadata": {
        "id": "52AKgo9oYzO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b8acd-ef88-411c-dd51-a02d2dbc2803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-py3-none-any.whl size=14727025 sha256=c5c608112a1838ff85dcbd39dda715c38532a4568b479f986e864fc2c0ab1f0f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5i3izlmo/wheels/c9/a6/ea/0778337c34660027ee67ef3a91fb9d3600b76777a912ea1c24\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Colab Runtime\n",
        "Click menu Runtime - Restart runtime"
      ],
      "metadata": {
        "id": "fV4MJkKIwz6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IWSLT2016 Dataset"
      ],
      "metadata": {
        "id": "6fi66Uck3y9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import IWSLT2016\n",
        "from typing import Iterable, List\n",
        "import numpy as np\n",
        "\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"TorchText Version: {torchtext.__version__}\")"
      ],
      "metadata": {
        "id": "vS6tEAVjY9hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2a15db-c211-4ccb-d35d-d3642947fce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 1.10.0+cu111\n",
            "TorchText Version: 0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ERROR ALERT:\n",
        "*   You may receive a **\"RuntimeError: Internal error: confirm_token was not found in Google drive link.\"** in the cell below.\n",
        "*   This is a recent error in Torchtext 0.11.0 due to the dataset URL changing and/or changes to requests library.\n",
        "* We are trying to find a workaround.\n",
        "\n",
        "\n",
        "# Code that follows remains for illustrative purposes."
      ],
      "metadata": {
        "id": "9t5v3gJyTbAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WARNING:\n",
        "If you get an error running the next cell, it's probably because you didn't restart the runtime in step 1."
      ],
      "metadata": {
        "id": "MqwVE35Sv64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = IWSLT2016(split=('train'), \n",
        "                       language_pair=('en','fr'))"
      ],
      "metadata": {
        "id": "ffnXL02wUoVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = get_tokenizer('spacy',language='en_core_web_sm')\n",
        "tokenizer_fr = get_tokenizer('spacy',language='fr_core_news_sm')"
      ],
      "metadata": {
        "id": "6ZuMXQcDUvy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sent_len_en = []\n",
        "sent_len_fr = []\n",
        "iter_len = 0\n",
        "for en_sent, fr_sent in train_iter:\n",
        "  sent_len_en.append(len(tokenizer_en(en_sent)))\n",
        "  sent_len_fr.append(len(tokenizer_fr(fr_sent)))\n",
        "  iter_len += 1\n",
        "print(f'Dataset contains {iter_len} sentences.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buxZZErQZNYY",
        "outputId": "459bebd0-f091-47bf-e9ef-c60a3af56767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 220400 sentences.\n",
            "CPU times: user 1min 8s, sys: 355 ms, total: 1min 8s\n",
            "Wall time: 1min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check sentence lengths in dataset\n",
        "Are the buckets sizes good?"
      ],
      "metadata": {
        "id": "4oay4E7q4A01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_sizes = [(5, 10), (10, 15), (20, 25), (40, 50)]"
      ],
      "metadata": {
        "id": "XkV01PwRYAbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(np.array(sent_len_en).min(), np.array(sent_len_en).max())\n",
        "plt.hist(sent_len_en, range=(0,100))\n",
        "\n",
        "print(np.array(sent_len_fr).min(), np.array(sent_len_fr).max())\n",
        "plt.figure\n",
        "plt.hist(sent_len_fr, range=(0,100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "GgIaq2GpctYI",
        "outputId": "bbfa4f83-4df5-439e-c73a-308d7031df7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 754\n",
            "2 887\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([29694., 85086., 51610., 27633., 13436.,  6483.,  3045.,  1559.,\n",
              "          803.,   422.]),\n",
              " array([  0.,  10.,  20.,  30.,  40.,  50.,  60.,  70.,  80.,  90., 100.]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocabularies (en, fr)"
      ],
      "metadata": {
        "id": "MiFsVsYH33g0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter, language):\n",
        "    if language == 'en':\n",
        "      for data_sample in data_iter:\n",
        "          yield tokenizer_en(data_sample[0])\n",
        "    else:\n",
        "      for data_sample in data_iter:\n",
        "        yield tokenizer_fr(data_sample[1])\n",
        "\n",
        "UNK_IDX, PAD_IDX, GO_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<go>', '<eos>']\n",
        "\n",
        "# Create Vocabs\n",
        "train_iter = IWSLT2016(root='.data', split=('train'), \n",
        "                             language_pair=('en', 'fr'))\n",
        "\n",
        "vocab_en = build_vocab_from_iterator(\n",
        "                  yield_tokens(train_iter, 'en'),\n",
        "                  min_freq=1,\n",
        "                  specials=special_symbols,\n",
        "                  special_first=True)\n",
        "\n",
        "train_iter = IWSLT2016(root='.data', split=('train'), \n",
        "                             language_pair=('en', 'fr'))\n",
        "vocab_fr = build_vocab_from_iterator(\n",
        "                  yield_tokens(train_iter, 'fr'),\n",
        "                  min_freq=1,\n",
        "                  specials=special_symbols,\n",
        "                  special_first=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_-IjjNj2UrZ",
        "outputId": "205824cd-f774-44e0-a845-f67d520106a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 188M/188M [00:01<00:00, 133MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab_en), len(vocab_fr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI30VfRd3pRV",
        "outputId": "195e9bf2-1ad3-4425-acd6-89d86b224184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61911 81272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "BoMR-GjY4NRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Token processing:\n",
        "#   source: pad and reverse\n",
        "#   target: add GO, EOS, and pad\n",
        "\n",
        "def process_tokens(source, target, bucket_sizes):\n",
        "  # find bucket_index\n",
        "  for i in range(len(bucket_sizes)+2):\n",
        "    # truncate if we exhauset list of buckets\n",
        "    if i >= len(bucket_sizes): \n",
        "      bucket = bucket_sizes[i-1]\n",
        "      bucket_id = i-1\n",
        "      if len(source) > bucket[0]:\n",
        "        source = source[:bucket[0]]\n",
        "      if len(target) > (bucket[1]-2):\n",
        "        target = target[:bucket[1]-2]\n",
        "      break\n",
        "\n",
        "    bucket = bucket_sizes[i]\n",
        "    if (len(source) < bucket[0]) and ((len(target)+1) < bucket[1]):\n",
        "      bucket_id = i\n",
        "      break\n",
        "  \n",
        "  source += ((bucket_sizes[bucket_id][0] - len(source)) * ['<pad>'])\n",
        "  source = list(reversed(source))\n",
        "\n",
        "  target.insert(0,'<go>')\n",
        "  target.append('<eos>')\n",
        "  target += (bucket_sizes[bucket_id][1] - len(target)) * ['<pad>']\n",
        "\n",
        "  return vocab_en(source), vocab_fr(target), bucket_id\n",
        "\n",
        "  # source_t = torch.tensor(vocab_en(source), dtype=torch.float32)\n",
        "  # target_t = torch.tensor(vocab_fr(target), dtype=torch.float32)\n",
        "  # return source_t, target_t, bucket_id"
      ],
      "metadata": {
        "id": "h84E22mynCjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test process_tokens()\n",
        "train_iter = IWSLT2016(split=('train'), language_pair=('en','fr'))\n",
        "for sent_en, sent_fr  in train_iter:\n",
        "  source, target, bucket_id = process_tokens(tokenizer_en(sent_en), \n",
        "                                  tokenizer_fr(sent_fr), \n",
        "                                  bucket_sizes)\n",
        "  print(bucket_id, bucket_sizes[bucket_id])\n",
        "  print(len(source), source)\n",
        "  print(len(target),target)\n",
        "  # print(source.shape, source)\n",
        "  # print(target.shape,target)\n",
        "\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zMLvgPPojjr",
        "outputId": "000d4743-b570-4794-bbb1-106b04078813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 (20, 25)\n",
            "20 [1, 1, 1, 1, 1, 1, 5, 6, 26234, 6842, 89, 13, 6, 26487, 2039, 15, 96, 55, 26234, 1518]\n",
            "25 [2, 1696, 31203, 47, 182, 2265, 31468, 5, 52, 90, 7916, 31203, 5, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bucketed_datasets(data_iter, bucket_sizes, max_data_size=None):\n",
        "  datasets = []\n",
        "  for i in range(len(bucket_sizes)):\n",
        "    datasets.append([])\n",
        "\n",
        "  iter_len = 0\n",
        "  for sent_en, sent_fr  in train_iter:\n",
        "    source, target, bucket_id = process_tokens(tokenizer_en(sent_en), \n",
        "                                    tokenizer_fr(sent_fr), \n",
        "                                    bucket_sizes)\n",
        "    datasets[bucket_id].append((source, target))\n",
        "    iter_len += 1\n",
        "    if max_data_size != None and iter_len > max_data_size:\n",
        "      break\n",
        "    #break\n",
        "  print(f'Dataset contains {iter_len} sentences.')\n",
        "  return datasets"
      ],
      "metadata": {
        "id": "_OtEofGqfaqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test create_bucketed_datasets()\n",
        "train_iter = IWSLT2016(split=('train'), language_pair=('en','fr'))\n",
        "datasets = create_bucketed_datasets(train_iter, bucket_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaACi58B6PHh",
        "outputId": "a586fad2-7964-40d9-de5c-7a26d06006f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 220400 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets:\n",
        "  print(len(dataset), len(dataset[0][0]), len(dataset[0][1]))\n",
        "datasets[0][0][1] # dataset, sample, token_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiDzqkp39ns1",
        "outputId": "5f97a393-4d85-4298-bd14-b2474bc9f4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2047 5 10\n",
            "30789 10 15\n",
            "87726 20 25\n",
            "99838 40 50\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 194, 5, 6, 3, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataloaders for batching"
      ],
      "metadata": {
        "id": "DN9j3GmQ8I0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When we get a batch, the data should be same bucket size\n",
        "# Create separate Map-Style datasets based on bucket size\n",
        "# Each dataset will have a separate Dataloader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BucketedDataset(Dataset):\n",
        "  def __init__(self, bucketed_dataset, bucket_size):\n",
        "    super(BucketedDataset, self).__init__()\n",
        "    self.length = len(bucketed_dataset)\n",
        "    self.input_len = bucket_size[0]\n",
        "    self.target_len = bucket_size[1]\n",
        "    self.bucketed_dataset = bucketed_dataset\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (torch.tensor(self.bucketed_dataset[index][0], \n",
        "                         dtype=torch.float32),\n",
        "            torch.tensor(self.bucketed_dataset[index][1], \n",
        "                         dtype=torch.float32))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n"
      ],
      "metadata": {
        "id": "CYFgAn2vYqnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucketed_datasets = []\n",
        "for i, dataset in enumerate(datasets):\n",
        "  bucketed_datasets.append(BucketedDataset(dataset, \n",
        "                                           bucket_sizes[i]))\n",
        "# for dataset in bucketed_datasets:\n",
        "#   print(len(dataset))\n",
        "#   print(dataset[0])"
      ],
      "metadata": {
        "id": "KS5b3_sa8b4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Dataloaders for batching"
      ],
      "metadata": {
        "id": "8h_iPeU0E9Ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "57GrocqTFHEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = []\n",
        "for dataset in bucketed_datasets:\n",
        "  dataloaders.append(DataLoader(dataset, \n",
        "                                batch_size=32, \n",
        "                                shuffle=True))"
      ],
      "metadata": {
        "id": "GTITMQ_g0_Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "3mQkttl8FYQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TranslateLSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TranslateLSTM, self).__init__()\n",
        "    self.layer = nn.Linear(10,1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)"
      ],
      "metadata": {
        "id": "sJST9iruLhLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = TranslateLSTM()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def train(source, target):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(inputs)\n",
        "  loss = loss_fn(outputs, inputs)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss, output\n",
        "\n"
      ],
      "metadata": {
        "id": "10C8LMzRMIei"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}