{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch13_02_RL_DQN_Breakout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning and Deep Q-Networks"
      ],
      "metadata": {
        "id": "zBjJQ8BYnUBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9yBALdXb7PP"
      },
      "outputs": [],
      "source": [
        "# Download Atari ROMS for Breakout\n",
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing Breakout wth DQN\n"
      ],
      "metadata": {
        "id": "kYdVJOj0nYfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "# for reproductability\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'AI Gym: {gym.__version__}')\n",
        "print(f'Numpy: {np.__version__}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvbW_dQcHVs",
        "outputId": "07895332-2337-4b2b-aa46-54b1e127c4e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 1.10.0+cu111\n",
            "AI Gym: 0.17.3\n",
            "Numpy: 1.21.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_action(action_distribution, epsilon=1e-5):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    if random.random() < epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "           action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)\n",
        "\n",
        "def epsilon_greedy_action_annealed(action_distribution,\n",
        "                                   percentage, \n",
        "                                   epsilon_start=1.0, \n",
        "                                   epsilon_end=1e-8):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    annealed_epsilon = epsilon_start*(1.0-percentage) + epsilon_end*percentage\n",
        "    if random.random() < annealed_epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "          action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)"
      ],
      "metadata": {
        "id": "kSMsXEoHcYP4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodeHistory(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.terminals = []\n",
        "\n",
        "    def add_to_history(self, state, action, reward, \n",
        "      state_prime, terminal):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.state_primes.append(state_prime)\n",
        "        self.terminals.append(terminal)"
      ],
      "metadata": {
        "id": "pJ2U61P7eOCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build DQN Agent"
      ],
      "metadata": {
        "id": "4w_wPjzsicA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent(object):\n",
        "\n",
        "    def __init__(self, num_actions,\n",
        "                 learning_rate=1e-3, history_length=4,\n",
        "                 screen_height=84, screen_width=84, \n",
        "                 gamma=0.99):\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.history_length = history_length\n",
        "        self.screen_height = screen_height\n",
        "        self.screen_width = screen_width\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.build_prediction_network()\n",
        "        self.build_target_network()\n",
        "        #self.build_training()\n",
        "    \n",
        "    def build_prediction_network(self):\n",
        "        self.model_predict = nn.Sequential(\n",
        "          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n",
        "          nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "          nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(3136, 512),\n",
        "          nn.Linear(512, self.num_actions)\n",
        "          )\n",
        "\n",
        "    def build_target_network(self):\n",
        "        self.model_target = nn.Sequential(\n",
        "          nn.Conv2d(4, 32, kernel_size=8 , stride=4),\n",
        "          nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "          nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(3136, 512),\n",
        "          nn.Linear(512, self.num_actions)\n",
        "          )\n",
        "\n",
        "    def sample_and_train_pred(self, replay_table, batch_size):\n",
        "\n",
        "        s_t, action, reward, s_t_plus_1, terminal = replay_table.sample_batch(\n",
        "              batch_size)\n",
        "\n",
        "        # given state_t, find q_t (predict_model) and q_t+1 (target_model)\n",
        "        # do it in batches\n",
        "        # Find q_t_plus_1\n",
        "        input_t = torch.from_numpy(s_t_plus_1).float()\n",
        "        model_t = self.model_target.float()\n",
        "        q_t_plus_1 = model_t(input_t)\n",
        "        \n",
        "        terminal = torch.tensor(terminal).float()\n",
        "        max_q_t_plus_1, _ = torch.max(q_t_plus_1, dim=1)\n",
        "        reward = torch.from_numpy(reward).float()\n",
        "        target_q_t = (1. - terminal) * self.gamma * max_q_t_plus_1 + reward\n",
        "\n",
        "        # Find q_t, and q_of_action\n",
        "        input_p = torch.from_numpy(s_t).float()\n",
        "        model_p = self.model_predict.float()\n",
        "        q_t = model_p(input_p)\n",
        "        action = torch.from_numpy(action)\n",
        "        action_one_hot = nn.functional.one_hot(action, \n",
        "                                               self.num_actions)\n",
        "        q_of_action = torch.sum(q_t * action_one_hot)\n",
        "\n",
        "        # Compute loss\n",
        "        self.delta = (target_q_t - q_of_action)\n",
        "        self.loss = torch.mean(self.delta)\n",
        "\n",
        "        # Update predict_model gradients (only)\n",
        "        self.optimizer = optim.Adam(self.model_predict.parameters(),\n",
        "                                    lr = self.learning_rate)\n",
        "        self.loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return q_t\n",
        "\n",
        "    def predict_action(self, state, epsilon_percentage):\n",
        "        input_p = torch.from_numpy(state).float().unsqueeze(dim=0)\n",
        "        model_p = self.model_predict.float()\n",
        "        action_distribution = model_p(input_p)\n",
        "        # sample from action distribution\n",
        "        action = epsilon_greedy_action_annealed(action_distribution.detach(), \n",
        "                                                epsilon_percentage)\n",
        "        return action\n",
        "     \n",
        "    def process_state_into_stacked_frames(self, \n",
        "                                          frame, \n",
        "                                          past_frames, \n",
        "                                          past_state=None):\n",
        "        full_state = np.zeros((self.history_length, \n",
        "                              self.screen_width, \n",
        "                              self.screen_height))\n",
        "\n",
        "        if past_state is not None:\n",
        "            for i in range(len(past_state)-1):\n",
        "                full_state[i, :, :] = past_state[i+1, :, :]\n",
        "            full_state[-1, :, :] = self.preprocess_frame(frame, \n",
        "                                                        (self.screen_width, \n",
        "                                                          self.screen_height)\n",
        "                                                        )\n",
        "        else:\n",
        "            all_frames = past_frames + [frame]\n",
        "            for i, frame_f in enumerate(all_frames):\n",
        "                full_state[i, :, :] = self.preprocess_frame(frame_f, \n",
        "                                                            (self.screen_width, \n",
        "                                                            self.screen_height)\n",
        "                                                            )\n",
        "        return full_state\n",
        "\n",
        "    def to_grayscale(self, x):\n",
        "        return np.dot(x[...,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "    def preprocess_frame(self, im, shape):\n",
        "        cropped = im[16:201,:] # (185, 160, 3)\n",
        "        grayscaled = self.to_grayscale(cropped) # (185, 160)\n",
        "        # resize to (84,84)\n",
        "        resized = np.array(Image.fromarray(grayscaled).resize(shape))\n",
        "        mean, std = 40.45, 64.15\n",
        "        frame = (resized-mean)/std\n",
        "        return frame"
      ],
      "metadata": {
        "id": "Zqdz6C0OehUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Experience Replay"
      ],
      "metadata": {
        "id": "O06XH_jpilB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayTable(object):\n",
        "\n",
        "    def __init__(self, table_size=50000):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.terminals = []\n",
        "\n",
        "        self.table_size = table_size\n",
        "\n",
        "    def add_episode(self, episode):\n",
        "        self.states += episode.states\n",
        "        self.actions += episode.actions\n",
        "        self.rewards += episode.rewards\n",
        "        self.state_primes += episode.state_primes\n",
        "        self.terminals += episode.terminals\n",
        "\n",
        "        self.purge_old_experiences()\n",
        "\n",
        "    def purge_old_experiences(self):\n",
        "        while len(self.states) > self.table_size:\n",
        "            self.states.pop(0)\n",
        "            self.actions.pop(0)\n",
        "            self.rewards.pop(0)\n",
        "            self.state_primes.pop(0)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        s_t, action, reward, s_t_plus_1, terminal = [], [], [], [], []\n",
        "        rands = np.arange(len(self.states))\n",
        "        np.random.shuffle(rands)\n",
        "        rands = rands[:batch_size]\n",
        "\n",
        "        for r_i in rands:\n",
        "            s_t.append(self.states[r_i])\n",
        "            action.append(self.actions[r_i])\n",
        "            reward.append(self.rewards[r_i])\n",
        "            s_t_plus_1.append(self.state_primes[r_i])\n",
        "            terminal.append(self.terminals[r_i])\n",
        "        return (np.array(s_t), np.array(action), np.array(reward), \n",
        "                np.array(s_t_plus_1), np.array(terminal))"
      ],
      "metadata": {
        "id": "y7LaP4DHipXw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Q Learning\n",
        "- The configuration values below are for illustrative purposes so you can execute the code in Colab. Training takes a long time (days) at realistic values.\n",
        "- A larger reward value for Breakout may require max_episode_length to be 100000.  That is, you need to be able to play long enough to get a decent reward (score). "
      ],
      "metadata": {
        "id": "EkSGh7c_hEUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn_start = 4\n",
        "total_episodes = 32\n",
        "epsilon_stop = 32\n",
        "train_frequency = 2\n",
        "target_frequency = 4\n",
        "batch_size = 4\n",
        "max_episode_length = 1000\n",
        "env = gym.make('Breakout-v4')\n",
        "num_actions = env.action_space.n\n",
        "solved = False"
      ],
      "metadata": {
        "id": "w3VlHF6m3GjG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(num_actions=num_actions, \n",
        "                 learning_rate=1e-4, \n",
        "                 history_length=4,\n",
        "                 gamma=0.98)"
      ],
      "metadata": {
        "id": "yv5hfhFBhO7p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train DQN"
      ],
      "metadata": {
        "id": "VgkJn_-8h_EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_rewards = []\n",
        "q_t_list = []\n",
        "batch_losses = []\n",
        "past_frames_last_time = None\n",
        "\n",
        "replay_table = ExperienceReplayTable()\n",
        "global_step_counter = 0\n",
        "\n",
        "for i in range(total_episodes):\n",
        "    # Get initial frame -> state\n",
        "    frame = env.reset() # np.array of shape (210, 160, 3)\n",
        "    # past_frames is a list of past 3 frames (np.arrays)\n",
        "    past_frames = [copy.deepcopy(frame) for _ in range(agent.history_length-1)]\n",
        "    state = agent.process_state_into_stacked_frames(\n",
        "        frame, past_frames, past_state=None) # state is (4,84,84)\n",
        "    \n",
        "    # initialize episode history (s_t, a, r, s_t+1, terminal)\n",
        "    episode_reward = 0.0\n",
        "    episode_history = EpisodeHistory()\n",
        "    epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\n",
        "\n",
        "    for j in range(max_episode_length):\n",
        "        # predict action or choose random action at first\n",
        "        if global_step_counter < learn_start:\n",
        "          action = np.argmax(np.random.random((agent.num_actions)))\n",
        "        else:\n",
        "          action = agent.predict_action(state, epsilon_percentage)\n",
        "\n",
        "        # take action, get next frame (-> next state), reward, and terminal\n",
        "        reward = 0\n",
        "        frame_prime, reward, terminal, _ = env.step(action)\n",
        "        if terminal == True:\n",
        "          reward -= 1\n",
        "          \n",
        "        # get next state from next frame and past frames \n",
        "        state_prime = agent.process_state_into_stacked_frames(frame_prime, \n",
        "                                                              past_frames, \n",
        "                                                              past_state=state)\n",
        "        # Update past_frames with frame_prime for next time\n",
        "        past_frames.append(frame_prime)\n",
        "        past_frames = past_frames[len(past_frames)-agent.history_length:]\n",
        "        past_frames_last_time = past_frames\n",
        "\n",
        "        # Add to episode history (state, action, reward, state_prime, terminal)\n",
        "        episode_history.add_to_history(\n",
        "                    state, action, reward, state_prime, terminal)\n",
        "        state = state_prime\n",
        "        episode_reward += reward\n",
        "        global_step_counter += 1\n",
        "        \n",
        "        #  Do not train predict_model until we have enough \n",
        "        #   episodes in episode history\n",
        "        if global_step_counter > learn_start:\n",
        "          if global_step_counter % train_frequency == 0:\n",
        "              if(len(replay_table.actions) != 0):\n",
        "                q_t = agent.sample_and_train_pred(replay_table, batch_size)\n",
        "                q_t_list.append(q_t)\n",
        "\n",
        "                if global_step_counter % target_frequency == 0:\n",
        "                    agent.model_target.load_state_dict(\n",
        "                        agent.model_predict.state_dict())\n",
        "\n",
        "        # If terminal or max episodes reached, \n",
        "        #   add episode_history to replay table\n",
        "        if j == (max_episode_length - 1):\n",
        "            terminal = True\n",
        "\n",
        "        if terminal:\n",
        "            replay_table.add_episode(episode_history)\n",
        "            episode_rewards.append(episode_reward)\n",
        "            break\n",
        "    print(f'Episode[{i}]: {len(episode_history.actions)} \\\n",
        "              actions {episode_reward} reward')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi_y4JxhhTP6",
        "outputId": "fc2bb3dd-6397-4810-a432-c0ce1893c5fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode[0]: 306               actions 2.0 reward\n",
            "Episode[1]: 228               actions 0.0 reward\n",
            "Episode[2]: 172               actions -1.0 reward\n",
            "Episode[3]: 231               actions 0.0 reward\n",
            "Episode[4]: 266               actions 0.0 reward\n",
            "Episode[5]: 223               actions 0.0 reward\n",
            "Episode[6]: 280               actions 1.0 reward\n",
            "Episode[7]: 205               actions -1.0 reward\n",
            "Episode[8]: 188               actions -1.0 reward\n",
            "Episode[9]: 388               actions 3.0 reward\n",
            "Episode[10]: 239               actions 0.0 reward\n",
            "Episode[11]: 216               actions 0.0 reward\n",
            "Episode[12]: 230               actions 0.0 reward\n",
            "Episode[13]: 246               actions 0.0 reward\n",
            "Episode[14]: 288               actions 1.0 reward\n",
            "Episode[15]: 183               actions -1.0 reward\n",
            "Episode[16]: 214               actions -1.0 reward\n",
            "Episode[17]: 253               actions 1.0 reward\n",
            "Episode[18]: 259               actions 0.0 reward\n",
            "Episode[19]: 169               actions -1.0 reward\n",
            "Episode[20]: 216               actions -1.0 reward\n",
            "Episode[21]: 461               actions 4.0 reward\n",
            "Episode[22]: 228               actions 0.0 reward\n",
            "Episode[23]: 346               actions 2.0 reward\n",
            "Episode[24]: 315               actions -1.0 reward\n",
            "Episode[25]: 293               actions -1.0 reward\n",
            "Episode[26]: 284               actions 0.0 reward\n",
            "Episode[27]: 367               actions 0.0 reward\n",
            "Episode[28]: 327               actions 0.0 reward\n",
            "Episode[29]: 437               actions 0.0 reward\n",
            "Episode[30]: 342               actions 0.0 reward\n",
            "Episode[31]: 872               actions 0.0 reward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7qvr9aKy_WA5"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}