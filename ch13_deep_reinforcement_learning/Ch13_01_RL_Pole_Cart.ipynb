{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch13_01_RL_Pole_Cart.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Processes (MDP)\n",
        "## Future Return"
      ],
      "metadata": {
        "id": "-yGp0pP7LOgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# for reproductability\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ],
      "metadata": {
        "id": "p_lTuUjY_QLg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_naive_returns(rewards):\n",
        "    \"\"\" Calculates a list of naive returns given a \n",
        "    list of rewards.\"\"\"\n",
        "    total_returns = np.zeros(len(rewards))\n",
        "    total_return = 0.0\n",
        "    for t in range(len(rewards), 0):\n",
        "        total_return = total_return + reward\n",
        "        total_returns[t] = total_return\n",
        "    return total_returns\n",
        "\n",
        "\n",
        "def discount_rewards(rewards, gamma=0.98):\n",
        "    discounted_returns = [0 for _ in rewards]\n",
        "    discounted_returns[-1] = rewards[-1]\n",
        "    # iterate backwards\n",
        "    for t in range(len(rewards)-2, -1, -1): \n",
        "        discounted_returns[t] = (rewards[t] + \n",
        "              discounted_returns[t+1]*gamma)\n",
        "    return discounted_returns\n",
        "\n",
        "def epsilon_greedy_action(action_distribution, \n",
        "                          epsilon=1e-1):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    if random.random() < epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "           action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)\n",
        "\n",
        "def epsilon_greedy_action_annealed(action_distribution,\n",
        "                                   percentage, \n",
        "                                   epsilon_start=1.0, \n",
        "                                   epsilon_end=1e-2):\n",
        "    action_distribution = action_distribution.detach().numpy()\n",
        "    annealed_epsilon = (epsilon_start*(1.0-percentage) + \n",
        "                        epsilon_end*percentage)\n",
        "    if random.random() < annealed_epsilon:\n",
        "        return np.argmax(np.random.random(\n",
        "          action_distribution.shape))\n",
        "    else:\n",
        "        return np.argmax(action_distribution)"
      ],
      "metadata": {
        "id": "FzfQoWzz_RkV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pole-Cart with Policy Gradients"
      ],
      "metadata": {
        "id": "PyufQkgboZY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Agent"
      ],
      "metadata": {
        "id": "1BR54huUoHOG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjQX09lMWloJ"
      },
      "source": [
        "from torch import optim\n",
        "class PGAgent(object):\n",
        "    def __init__(self, state_size, num_actions, \n",
        "                 hidden_size, \n",
        "                 learning_rate=1e-3, \n",
        "                 explore_exploit_setting= \\\n",
        "                 'epsilon_greedy_annealed_1.0->0.001'):\n",
        "        self.state_size = state_size\n",
        "        self.num_actions = num_actions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.explore_exploit_setting = \\\n",
        "                        explore_exploit_setting\n",
        "        self.build_model()\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "      self.model = torch.nn.Sequential(\n",
        "        nn.Linear(self.state_size, self.hidden_size),\n",
        "        nn.Linear(self.hidden_size, self.hidden_size),\n",
        "        nn.Linear(self.hidden_size, self.num_actions),\n",
        "        nn.Softmax(dim=0))\n",
        "\n",
        "    def train(self, state, action_input, reward_input):\n",
        "        state = torch.tensor(state).float()\n",
        "        action_input = torch.tensor(action_input).long()\n",
        "        reward_input = torch.tensor(reward_input).float()\n",
        "        self.output = self.model(state)\n",
        "        # Select the logits related to the action taken\n",
        "        logits_for_actions = self.output.gather(1,action_input.view(-1,1))\n",
        "\n",
        "        self.loss = -torch.mean(\n",
        "            torch.log(logits_for_actions) * reward_input)\n",
        "        self.loss.backward()\n",
        "        self.optimizer = optim.Adam(self.model.parameters())\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return self.loss.item()\n",
        "        \n",
        "\n",
        "\n",
        "    def sample_action_from_distribution(self, \n",
        "                                        action_distribution, \n",
        "                                        epsilon_percentage):\n",
        "        # Choose an action based on the action probability\n",
        "        # distribution and an explore vs exploit\n",
        "        if self.explore_exploit_setting == 'greedy':\n",
        "              action = epsilon_greedy_action(action_distribution,0.00)\n",
        "        elif self.explore_exploit_setting == 'epsilon_greedy_0.05':\n",
        "              action = epsilon_greedy_action(action_distribution,0.05)\n",
        "        elif self.explore_exploit_setting == 'epsilon_greedy_0.25':\n",
        "              action = epsilon_greedy_action(action_distribution,0.25)\n",
        "        elif self.explore_exploit_setting == 'epsilon_greedy_0.50':\n",
        "              action = epsilon_greedy_action(action_distribution,0.50)\n",
        "        elif self.explore_exploit_setting == 'epsilon_greedy_0.90':\n",
        "              action = epsilon_greedy_action(action_distribution,0.90)\n",
        "        elif self.explore_exploit_setting == \\\n",
        "          'epsilon_greedy_annealed_1.0->0.001':\n",
        "              action = epsilon_greedy_action_annealed(\n",
        "                  action_distribution, \n",
        "                  epsilon_percentage, 1.0,0.001)\n",
        "        elif self.explore_exploit_setting == \\\n",
        "          'epsilon_greedy_annealed_0.5->0.001':\n",
        "              action = epsilon_greedy_action_annealed(\n",
        "                  action_distribution, \n",
        "                  epsilon_percentage, 0.5, 0.001)\n",
        "        elif self.explore_exploit_setting == \\\n",
        "          'epsilon_greedy_annealed_0.25->0.001':\n",
        "              action = epsilon_greedy_action_annealed(\n",
        "                  action_distribution, \n",
        "                  epsilon_percentage, 0.25, 0.001)\n",
        "        return action\n",
        "\n",
        "    def predict_action(self, state, epsilon_percentage):\n",
        "        action_distribution = self.model(torch.from_numpy(state).float())\n",
        "        action = self.sample_action_from_distribution(\n",
        "            action_distribution, epsilon_percentage)\n",
        "        return action"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keeping Track of History"
      ],
      "metadata": {
        "id": "TfvvUskNohYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpisodeHistory(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.discounted_returns = []\n",
        "\n",
        "    def add_to_history(self, state, action, reward, \n",
        "      state_prime):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.state_primes.append(state_prime)\n",
        "\n",
        "\n",
        "class Memory(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.discounted_returns = []\n",
        "\n",
        "    def reset_memory(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.state_primes = []\n",
        "        self.discounted_returns = []\n",
        "\n",
        "    def add_episode(self, episode):\n",
        "        self.states += episode.states\n",
        "        self.actions += episode.actions\n",
        "        self.rewards += episode.rewards\n",
        "        self.discounted_returns += episode.discounted_returns"
      ],
      "metadata": {
        "id": "YAGE0IJR_uX_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient Main Function"
      ],
      "metadata": {
        "id": "xsRN8A9Momgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Settings\n",
        "#total_episodes = 5000\n",
        "total_episodes = 16\n",
        "total_steps_max = 10000\n",
        "epsilon_stop = 3000\n",
        "train_frequency = 8\n",
        "max_episode_length = 500\n",
        "render_start = -1\n",
        "should_render = False\n",
        "\n",
        "explore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001'\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "state_size = env.observation_space.shape[0]  # 4 for \n",
        "                                              # CartPole-v0\n",
        "num_actions = env.action_space.n  # 2 for CartPole-v0\n",
        "\n",
        "solved = False\n",
        "agent = PGAgent(state_size=state_size,\n",
        "                num_actions=num_actions,\n",
        "                hidden_size=16, \n",
        "                explore_exploit_setting= \\\n",
        "                  explore_exploit_setting)"
      ],
      "metadata": {
        "id": "_dhvNmQn4lh0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode_rewards = []\n",
        "batch_losses = []\n",
        "\n",
        "global_memory = Memory()\n",
        "steps = 0\n",
        "for i in range(total_episodes):\n",
        "  state = env.reset()\n",
        "  episode_reward = 0.0\n",
        "  episode_history = EpisodeHistory()\n",
        "  epsilon_percentage = float(min(i/float(epsilon_stop), 1.0))\n",
        "\n",
        "  for j in range(max_episode_length):\n",
        "      action = agent.predict_action(state, epsilon_percentage)\n",
        "      state_prime, reward, terminal, _ = env.step(action)\n",
        "      \n",
        "      episode_history.add_to_history(\n",
        "          state, action, reward, state_prime)\n",
        "      state = state_prime\n",
        "      episode_reward += reward\n",
        "      steps += 1\n",
        "      \n",
        "      if j == (max_episode_length - 1):\n",
        "            terminal = True\n",
        "            \n",
        "      if terminal:\n",
        "          episode_history.discounted_returns = \\\n",
        "            discount_rewards(episode_history.rewards)\n",
        "          global_memory.add_episode(episode_history)\n",
        "\n",
        "          # every 8th episode train the NN\n",
        "          # train on all actions from episodes in memory, then reset memory\n",
        "          if np.mod(i, train_frequency) == 0:\n",
        "            reward_input = global_memory.discounted_returns\n",
        "            action_input = global_memory.actions\n",
        "            state = global_memory.states\n",
        "\n",
        "            # train step \n",
        "            batch_loss = agent.train(state, action_input, reward_input)\n",
        "              # print(f'Batch loss: {batch_loss}')\n",
        "              # batch_losses.append(batch_loss)\n",
        "            global_memory.reset_memory()\n",
        "\n",
        "          episode_rewards.append(episode_reward)\n",
        "\n",
        "          if i % 10 == 0:\n",
        "              mean_rewards = torch.mean(torch.tensor(episode_rewards[:-10]))\n",
        "              if mean_rewards > 10.0:\n",
        "                  solved = True\n",
        "              else:\n",
        "                  solved = False\n",
        "              print(f'Solved: {solved} Mean Reward: {mean_rewards}')\n",
        "          break # stop playing if terminal\n",
        "          \n",
        "  print(f'Episode[{i}]: {len(episode_history.actions)} \\\n",
        "          actions {episode_reward} reward')"
      ],
      "metadata": {
        "id": "LS7KWo07_61i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f9256f3-b5fc-4db7-efd3-06068f47a654"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solved: False Mean Reward: nan\n",
            "Episode[0]: 11           actions 11.0 reward\n",
            "Episode[1]: 14           actions 14.0 reward\n",
            "Episode[2]: 14           actions 14.0 reward\n",
            "Episode[3]: 18           actions 18.0 reward\n",
            "Episode[4]: 22           actions 22.0 reward\n",
            "Episode[5]: 15           actions 15.0 reward\n",
            "Episode[6]: 17           actions 17.0 reward\n",
            "Episode[7]: 12           actions 12.0 reward\n",
            "Episode[8]: 38           actions 38.0 reward\n",
            "Episode[9]: 14           actions 14.0 reward\n",
            "Solved: True Mean Reward: 11.0\n",
            "Episode[10]: 15           actions 15.0 reward\n",
            "Episode[11]: 19           actions 19.0 reward\n",
            "Episode[12]: 16           actions 16.0 reward\n",
            "Episode[13]: 18           actions 18.0 reward\n",
            "Episode[14]: 19           actions 19.0 reward\n",
            "Episode[15]: 27           actions 27.0 reward\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mSEVscRR_y6w"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}